pub_date	title	venue	excerpt	citation	url_slug	paper_url
2022-06-25	PeruSIL: A Framework to Build a Continuous Peruvian Sign Language Interpretation Dataset	Proceedings of the LREC2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources	Video-based datasets for Continuous Sign Language are scarce due to the challenging task of recording videos from native signers and the reduced number of people who can annotate sign language. COVID-19 has evidenced the key role of sign language interpreters in delivering nationwide health messages to deaf communities. In this paper, we present a framework for creating a multi-modal sign language interpretation dataset based on videos and we use it to create the first dataset for Peruvian Sign Language (LSP) interpretation annotated by hearing volunteers who have intermediate knowledge of PSL guided by the video audio. We rely on hearing people to produce a first version of the annotations, which should be reviewed by native signers in the future. Our contributions: i) we design a framework to annotate a sign Language dataset; ii) we release the first annotated LSP multi-modal interpretation dataset (AEC); iii) we evaluate the annotation done by hearing people by training a sign language recognition model. Our model reaches up to 80.3% of accuracy among a minimum of five classes (signs) AEC dataset, and 52.4% in a second dataset. Nevertheless, analysis by subject in the second dataset show variations worth to discuss.	Bejarano, Gissella and Huamani-Malca, Joe and Cerna-Herrera, Francisco and Alva-Manchego, Fernando and Rivas, Pablo: PeruSIL: A Framework to Build a Continuous Peruvian Sign Language Interpretation Dataset. Proceedings of the LREC2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources pp. 1â€“8. European Language Resources As-sociation, Marseille, France (2022).	2022.signlang-1.1	https://aclanthology.org/2022.signlang-1.1.pdf
2022-11-28	Impact of Pose Estimation Models for Landmark-based Sign Language Recognition	Neural Information Processing Systems Conference: LatinX in AI (LXAI) Research Workshop 2022	Sign Language Recognition (SLR) models rely heavily on advances reached by Human Action Recognition (HAR). One of the simplest and most dimensionalreduced modalities is the skeleton joints and limbs represented with key-point landmarks and edges connecting these landmarks. These skeletons can be obtained by pose estimation, depth maps, or motion capture. For HAR, models are usually interested in less granularity of pose estimation compared to SLR, where the landmark estimation of not only the pose and body but the facial gestures, hands, and fingers is crucial. In this work, we compare three whole-body estimation libraries/models that are gaining attraction in the SLR task. We first find their relation by identifying common keypoints in their landmark structure and analyzing their quality. Then, we complement this analysis by comparing their annotations in three sign language datasets with videos of different quality, backgrounds, and regions (Peru and USA). Finally, we test a sign language recognition model to compare the quality of the annotations provided by these libraries/models.	Bejarano, G. et al., (2022). Impact of Pose Estimation Models for Landmark-based Sign Language Recognition [Poster Presentation]. Neural Information Processing Systems Conference: LatinX in AI (LXAI) Research Workshop 2022, New Orleans, USA.	impactPose2022	https://research.latinxinai.org/papers/neurips/2022/pdf/18_CameraReady.pdf
2023-06-18	Less is More: Techniques to Reduce Overfitting in your Transformer Model for Sign Language Recognition	Computer Vision and Pattern Recognition Conference: LatinX in AI (LXAI) Research Workshop 2023	Sign language recognition (SLR) in deep learning is a challenging task due to the need for interpreting human body movements, including detailed hand movements and facial expressions. Recent research has focused on using keypoint body landmarks and transformer models to improve SLR performance. However, these models can face overfitting issues due to the need for more available datasets. To address these problems, we analyze three Peruvian sign language (LSP) datasets for SLR. Additionally, we apply several techniques to reduce overfitting in the Spoter model, a transformer-based architecture for SLR. The results of these techniques reveal that the data-based techniques improve generalization and reduce overfitting in transformer-based models for SLR.	Huamani Malca, J. N. et al., (2023). Less is More: Techniques to Reduce Overfitting in your Transformer Model for Sign Language Recognition [Poster Presentation]. Computer Vision and Pattern Recognition Conference: LatinX in AI (LXAI) Research Workshop 2023, Vancouver, Canada.	lessIsMore-overfitting	https://research.latinxinai.org/papers/cvpr/2023/pdf/Joe_Huamani.pdf
2023-12-13	Comparing incremental learning approaches for a growing sign language dictionary	10th International Conference on Information Management and Big Data	Machine Learning-based Sign Language Dictionaries recognize a sign performed in front of a camera and return the most probable written language word. Due to the scarce number and variety of datasets for sign languages, these dictionaries need an incremental approach to include new signs each time a new dataset is available. Current sign language recognition models used in these dictionaries are trained for a fixed number of classes. For this reason, our work systematically compares three incremental learning approaches in a skeleton and transformer-based sign language recognition model to train up to 60 classes or Peruvian Sign Language (LSP) signs. In addition, we also evaluate two distinct incremental groups: only taking new classes and, taking new instance of old and new classes. This last incremental group is considerably less explored compared to the incremental approach involving only new classes at each step. We found that a method inspired by distillation loss outperforms others in most scenarios.	(to be published)	simbig2022-incrementalLearing	(to be published)